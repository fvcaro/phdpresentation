\def\layersep{1.5cm}

\begin{frame}{What is Deep Learning?}
\begin{minipage}[t]{0.48\linewidth}
\vspace{-0.1cm}
\begin{figure}
	\hspace{-1cm}
	\input{Diapos/Intro/Figures/DL_figure.tex}
\end{figure}
\end{minipage}
\begin{minipage}[t]{0.48\linewidth}
\visible<1-3>{\textcolor{jon_green}{Deep Learning (DL)} is part of a broader family of \textcolor{red}{Machine Learning (ML)} methods based on artificial Neural Networks. \textit{Wikipedia}}
\vspace{0.2cm}

\visible<2-3>{\textcolor{red}{Machine learning (ML)} is seen as a part of \textcolor{blue}{Artificial Intelligence (AI)} that is devoted to understanding and building methods that \textit{learn}. \textit{Wikipedia}}
\vspace{0.2cm}

\visible<3-3>{\textcolor{blue}{Artificial Intelligence (AI)} is the theory and development of computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, or decision-making. \textit{Oxford English Dictionary}}
\end{minipage}
\end{frame}


\begin{frame}{Deep Learning Methods}
\textbf{Advantages:}
\vspace{0.4cm}
\begin{itemize}
\item Affordable computational cost (\textit{High offline, low online})
\vspace{0.4cm}
\item Easily parallelizable implementation
\vspace{0.4cm}
\item Great approximation capabilities
\vspace{0.4cm}
\item Exploitable big data
\vspace{0.4cm}
\item Constant advances in computers
\vspace{0.4cm}
\item Trendy among the scientific community
\end{itemize}
\end{frame}


\begin{frame}[t]{Deep Learning Working Areas}
Language Processing
\begin{thebibliography}{1}
\bibitem{DL_lp} \small{D. Otter, J. Medina, and J. Kalita. A Survey of the Usages of Deep Learning for Natural Language Processing. IEEE Transactions on Neural Networks and Learning Systems, 32(2): 604-624, 2021.}
\end{thebibliography}
%https://ieeexplore.ieee.org/abstract/document/9075398?casa_token=un977of__0gAAAAA:f7OQhF6nUr9fNvTePPyrNz2_6Id2GxED9oyu6jtgdhnsaZxykeKdZHdvXJcNp6d0MBlqeY__
\vspace{0.5cm}

Image Processing
\begin{thebibliography}{2}
\bibitem{DL_image} \small{M. Egmont-Petersen, D. de Ridder, and H. Handels. Image processing with neural networks—a review. Pattern Recognition, 35(10): 2279-2301, 2002.}
\end{thebibliography}
%https://www.sciencedirect.com/science/article/abs/pii/S0031320301001789
\vspace{0.5cm}

Healthcare
\begin{thebibliography}{3}
\bibitem{DL_health} \small{A. Esteva et al. A guide to deep learning in healthcare. Nature Medicine, 25: 24–29, 2019.}
\end{thebibliography}
%https://www.nature.com/articles/s41591-018-0316-z?source=post_page---------------------------
\end{frame}


%\begin{frame}[t]{Artificial Neural Networks}
%\visible<1-2>{Artificial neural networks (also known in literature as Neural Networks) are computing systems inspired by the biological neural networks.}
%\vspace{0.3cm}
%
%They are composed of connected units called artificial neurons or nodes. 
%
%\vspace{0.3cm}
%\visible<2>{A node receives signals, then processes them and send the information to neurons connected to it.}
%
%\only<2>{
%\begin{figure}
%	\hspace{-2cm}
%	\input{Diapos/Intro/Figures/node.tex}
%\end{figure}
%}
%\end{frame}


\begin{frame}[t]{Artificial Neural Networks}\linespread{1.05}
\vspace{-.2 cm}
\begin{center}
	Approximate: ${\cal I} \approx {\cal I}_{\phi} := A_k \circ N \circ A_{k-1}  \circ \cdots \circ N \circ A_1 $\\
\end{center}
\only<1-2>{
	\vspace{0.2 cm}
	\centerline{$N$ -- Non-linear activation function  \hspace{0.3 cm};\hspace{0.3 cm} $A_k$ -- Affine transformation}
	\vspace{0.6cm}
\begin{figure}
	\vspace*{-0.25in}
	\input{Diapos/Intro/Figures/fully_connected_network.tex}
\end{figure}}
\only<3>{
	\vspace{0.4cm}
	
	$A_k$ -- Affine transformation: \hspace{0.1cm} $A_k \cdot x+b_k$
	\vspace{0.4cm}
	
	$N$ -- Non-linear activation function:

	\begin{figure}
	\centering
		\begin{subfigure}{.5\textwidth}
		\centering
		\input{Diapos/Intro/Figures/tanh.tex}
		%	\caption{Tanh}
		\label{fig:tanh}
		\end{subfigure}%
		\begin{subfigure}{.5\textwidth}
		\centering
		\input{Diapos/Intro/Figures/ReLU.tex}
		%	\caption{Tanh}
		\label{fig:tanh}
		\end{subfigure}
%	\caption{A figure with two subfigures}
	\label{fig:test}
	\end{figure}
	}
\end{frame}


\begin{frame}[t]{Deep Learning in Geophysics}
%They train a CNN to approximate the inverse operator. They use different metrics to control the value of the loss and use dropout method as regularization. 
%\begin{thebibliography}{4}
%\bibitem{geo_1} \small{D. Moghadas. One-dimensional deep learning inversion of electromagnetic induction data using convolutional neural network. Geophysical Journal International, 222(1): 247-259, 2020.}
%\end{thebibliography}
%https://academic.oup.com/gji/article-abstract/222/1/247/5818323
They train a CNN to approximate the inverse operator. They use the data misfit loss function with MSE metric and add a regularization term.
\begin{thebibliography}{5}
\bibitem{geo_2} \small{B. Liu et al. Deep Learning Inversion of Electrical Resistivity Data. IEEE Transactions on Geoscience and Remote Sensing, 58(8): 5715-5728, 2020.}
\end{thebibliography}
\vspace{0.2cm}

The model outputs several likely inverse solutions and it also predicts their probabilities. 
\begin{thebibliography}{5}
\bibitem{geo_2} \small{ S. Alyaev and A. H. Elsheikh. Direct multi-modal inversion of geophysical logs using deep learning. Earth and Space Science, 9, e2021EA002186, 2022.}
\end{thebibliography}
%https://ieeexplore.ieee.org/abstract/document/8994191?casa_token=dBQ0UUmOSyUAAAAA:vre7sUUy3QZ6OyaCxwigpHbmfR-HZdQWcXJ-Z0YMwDyMWXFnPISVJuEJlslqWLvuSZ1pvHa-iGvy
\vspace{0.2cm}

%En este utiliza un loss normal, el missfit, pero parte de una solucion a priori (o puede ser random tambien). Ellos no entrenan el inverso aqui, lo evaluan en el loss.
%\begin{thebibliography}{6}
%\bibitem{geo_3} \small{Y. Shi, X. Wu, and S. Fomel. Deep learning parameterization for geophysical inverse problems. SEG Global Meeting Abstracts, 36-40, 2020.}
%\end{thebibliography}
%%https://library.seg.org/doi/abs/10.1190/iwmg2019_09.1

\visible<2>{In the first part of this dissertation, \textbf{we analyze the behavior of different loss functions while solving inverse problems using Deep Learning.}
\begin{thebibliography}{6}
\bibitem{DL_me} \small{M. Shahriari, D. Pardo, J. A. Rivera et al. Error control and loss functions for the deep learning inversion of borehole resistivity measurements. International Journal for Numerical Methods in Engineering, 122(6): 1629-1657, 2021.}
\end{thebibliography}}
\end{frame}